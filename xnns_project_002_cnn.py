# -*- coding: utf-8 -*-
"""xNNs_Project_002_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14XS9aE5ld-fBnDOtpTBPGpf9-XoU2yFW
"""

################################################################################
#
# LOGISTICS
#
#    Max Xie
#    myx160030
#
# DESCRIPTION
#
#    Image classification in PyTorch for ImageNet reduced to 100 classes and
#    down sampled such that the short side is 64 pixels and the long side is
#    >= 64 pixels
#
#    This script achieved a best accuracy of 68.32% on epoch 50 with a learning
#    rate at that point of 0.000010 and time required for each epoch of ~ 110 s
#
# INSTRUCTIONS
#
#    1. Go to Google Colaboratory: https://colab.research.google.com/notebooks/welcome.ipynb
#    2. File - New Python 3 notebook
#    3. Cut and paste this file into the cell (feel free to divide into multiple cells)
#    4. Runtime - Run all
#
# NOTES
#
#    0. For a mapping of category names to directory names see:
#       https://gist.github.com/aaronpolhamus/964a4411c0906315deb9f4a3723aac57
#
#    1. The original 2012 ImageNet images are down sampled such that their short
#       side is 64 pixels (the other side is >= 64 pixels) and only 100 of the
#       original 1000 classes are kept.
#
#    2. Build and train a RegNetX image classifier modified as follows:
#
#       - Set stride = 1 (instead of stride = 2) in the stem
#       - Replace the first stride = 2 down sampling building block in the
#         original network by a stride = 1 normal building block
#       - The fully connected layer in the decoder outputs 100 classes instead
#         of 1000 classes
#
#       The original RegNetX takes in 3x224x224 input images and generates Nx7x7
#       feature maps before the decoder, this modified RegNetX will take in
#       3x56x56 input images and generate Nx7x7 feature maps before the decoder.
#       For reference, an implementation of this network took ~ 112 s per epoch
#       for training, validation and checkpoint saving on Sep 27, 2020 using a
#       free GPU runtime in Google Colab.
#
################################################################################

################################################################################
#
# IMPORT
#
################################################################################

# torch
import torch
import torch.nn       as     nn
import torch.optim    as     optim
from   torch.autograd import Function

# torch utils
import torchvision
import torchvision.transforms as transforms

# additional libraries
import os
import urllib.request
import zipfile
import time
import math
import numpy             as np
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

!pip install thop
from thop import profile, clever_format

################################################################################
#
# PARAMETERS
#
################################################################################

# data
DATA_DIR_1        = 'data'
DATA_DIR_2        = 'data/imagenet64'
DATA_DIR_TRAIN    = 'data/imagenet64/train'
DATA_DIR_TEST     = 'data/imagenet64/val'
DATA_FILE_TRAIN_1 = 'Train1.zip'
DATA_FILE_TRAIN_2 = 'Train2.zip'
DATA_FILE_TRAIN_3 = 'Train3.zip'
DATA_FILE_TRAIN_4 = 'Train4.zip'
DATA_FILE_TRAIN_5 = 'Train5.zip'
DATA_FILE_TEST_1  = 'Val1.zip'
DATA_URL_TRAIN_1  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train1.zip'
DATA_URL_TRAIN_2  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train2.zip'
DATA_URL_TRAIN_3  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train3.zip'
DATA_URL_TRAIN_4  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train4.zip'
DATA_URL_TRAIN_5  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train5.zip'
DATA_URL_TEST_1   = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Val1.zip'
DATA_BATCH_SIZE   = 512
DATA_NUM_WORKERS  = 4
DATA_NUM_CHANNELS = 3
DATA_NUM_CLASSES  = 100
DATA_RESIZE       = 64
DATA_CROP         = 56
DATA_MEAN         = (0.485, 0.456, 0.406)
DATA_STD_DEV      = (0.229, 0.224, 0.225)

# model
# add model parameters here
DEPTHS = [1,1,4,7]
WIDTHS = [24,56,152,368]
input = torch.randn(1, 3, 56, 56)

# add training parameters here
# training (linear warm up with cosine decay learning rate)
TRAINING_LR_MAX          = 0.001
TRAINING_LR_INIT_SCALE   = 0.01
TRAINING_LR_INIT_EPOCHS  = 5
TRAINING_LR_FINAL_SCALE  = 0.01
TRAINING_LR_FINAL_EPOCHS = 45
# TRAINING_LR_FINAL_EPOCHS = 5 # uncomment for a quick test
TRAINING_NUM_EPOCHS      = TRAINING_LR_INIT_EPOCHS + TRAINING_LR_FINAL_EPOCHS
TRAINING_LR_INIT         = TRAINING_LR_MAX*TRAINING_LR_INIT_SCALE
TRAINING_LR_FINAL        = TRAINING_LR_MAX*TRAINING_LR_FINAL_SCALE

# add file parameters here
# file
FILE_NAME = 'Proj2.pt'
FILE_SAVE = 1
FILE_LOAD = 0

################################################################################
#
# DATA
#
################################################################################

# create a local directory structure for data storage
if (os.path.exists(DATA_DIR_1) == False):
    os.mkdir(DATA_DIR_1)
if (os.path.exists(DATA_DIR_2) == False):
    os.mkdir(DATA_DIR_2)
if (os.path.exists(DATA_DIR_TRAIN) == False):
    os.mkdir(DATA_DIR_TRAIN)
if (os.path.exists(DATA_DIR_TEST) == False):
    os.mkdir(DATA_DIR_TEST)

# download data
if (os.path.exists(DATA_FILE_TRAIN_1) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_1, DATA_FILE_TRAIN_1)
if (os.path.exists(DATA_FILE_TRAIN_2) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_2, DATA_FILE_TRAIN_2)
if (os.path.exists(DATA_FILE_TRAIN_3) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_3, DATA_FILE_TRAIN_3)
if (os.path.exists(DATA_FILE_TRAIN_4) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_4, DATA_FILE_TRAIN_4)
if (os.path.exists(DATA_FILE_TRAIN_5) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_5, DATA_FILE_TRAIN_5)
if (os.path.exists(DATA_FILE_TEST_1) == False):
    urllib.request.urlretrieve(DATA_URL_TEST_1, DATA_FILE_TEST_1)

# extract data
with zipfile.ZipFile(DATA_FILE_TRAIN_1, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_2, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_3, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_4, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_5, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TEST_1, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TEST)

# transforms
transform_train = transforms.Compose([transforms.RandomResizedCrop(DATA_CROP), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])
transform_test  = transforms.Compose([transforms.Resize(DATA_RESIZE), transforms.CenterCrop(DATA_CROP), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])

# data sets
dataset_train = torchvision.datasets.ImageFolder(DATA_DIR_TRAIN, transform=transform_train)
dataset_test  = torchvision.datasets.ImageFolder(DATA_DIR_TEST,  transform=transform_test)

# data loader
dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=DATA_BATCH_SIZE, shuffle=True,  num_workers=DATA_NUM_WORKERS, pin_memory=True, drop_last=True)
dataloader_test  = torch.utils.data.DataLoader(dataset_test,  batch_size=DATA_BATCH_SIZE, shuffle=False, num_workers=DATA_NUM_WORKERS, pin_memory=True, drop_last=True)

def weight_initialization(m):
    """
      Initialize the weights of several different layers in the custom Class

      Initialize the weights of the Conv2d, BatchNorm2d, and Linear Layers

      Parameters:
          m (torch.nn): layer being passed through

      Returns:
          None
    """
    if isinstance(m, nn.Conv2d):
        p = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
        m.weight.data.normal_(mean=0.0, std = np.sqrt(2.0 / p))
    elif isinstance(m, nn.BatchNorm2d):
        m.weight.data.fill_(1.0)
        m.bias.data.zero_()
    elif isinstance(m, nn.Linear):
        m.weight.data.normal_(mean=0.0, std=0.01)
        m.bias.data.zero_()

def conv2d_cx(cx, in_channels, out_channels, kernel_size, stride=1, groups=1):
    """
    Computes the complexity of the conv2d Layer

    custom function that computes the MACs and Params of the Convolution Layer

    Parameters:
        cx (Dict[str,float]): dictionary of values to hold
        in_channels (int): number of channels at the input of the layer
        out_channels (int): number of channels at the output of the layer
        kernel_size (int): kernel_size by kernel_size filter size in the layer 
        stride (int): stride of the layer
        groups (int): number of groups in the layer

    Returns:
        returns Dictionary of updated values
    """
    h, w, macs, params = cx["h"], cx["w"], cx["macs"], cx["params"]
    h, w = ((h - 1) // stride) + 1, ((w - 1) // stride) + 1
    macs += (kernel_size * kernel_size * in_channels * out_channels * h * w) // (groups)
    params += (kernel_size * kernel_size * in_channels * out_channels) // (groups)
    print("MACs:",(kernel_size * kernel_size * in_channels * out_channels * h * w) // (groups),"PARAMS:", (kernel_size * kernel_size * in_channels * out_channels) // (groups))
    return {"h": h, "w": w, "macs": macs, "params": params}

################################################################################
#
# NETWORK BUILDING BLOCK
#
################################################################################

# X block
class XBlock(nn.Module):

    # initialization
    def __init__(self, in_channels, out_channels, stride, groups_num):
        # parent initialization
        super(XBlock, self).__init__()

        self.mismatch = False
        self.block_0 = None
        # add your code here
        # operations needed to create a parameterized XBlock
        if (in_channels != out_channels) or (stride != 1):
            self.mismatch = True
            self.block_0 = nn.ModuleList()
            self.block_0.append(nn.Conv2d(in_channels,out_channels,1,stride=stride))
            self.block_0.append(nn.BatchNorm2d(out_channels)) # ???????????

        self.block_1 = nn.ModuleList()
        self.block_1.append(nn.Conv2d(in_channels,out_channels,1))
        self.block_1.append(nn.BatchNorm2d(out_channels))
        self.block_1.append(nn.ReLU(inplace = True))

        self.block_2 = nn.ModuleList()
        self.block_2.append(nn.Conv2d(out_channels,out_channels,3,stride=stride, padding=(1,1), groups=groups_num))
        self.block_2.append(nn.BatchNorm2d(out_channels))
        self.block_2.append(nn.ReLU(inplace = True))

        self.block_3 = nn.ModuleList()
        self.block_3.append(nn.Conv2d(out_channels,out_channels,1))
        self.block_3.append(nn.BatchNorm2d(out_channels))

        self.relu = nn.ReLU(inplace = True)

    # forward path
    def forward(self, x):

        # add your code here
        # tie together the operations to create a parameterized XBlock
        res = x
        
        for layer in self.block_1:
            res = layer(res)
        
        for layer in self.block_2:
            res = layer(res)
        
        for layer in self.block_3:
            res = layer(res)

        if self.mismatch:
            for layer in self.block_0:
                x = layer(x)

        # return
        return self.relu(x + res)

################################################################################
#
# NETWORK
#
################################################################################

# define
class Model(nn.Module):

    # initialization
    def __init__(self,
                 data_num_channels,
                 # include model parameters here
                 depths, # DEPTHS = [1,1,4,7]    
                 widths, # WIDTHS = [24,56,152,368]
                 data_num_classes):

        # parent initialization
        super(Model, self).__init__()

        # add your code here
        # operations needed to create a modified RegNetX-200MF network
        # use the parameterized XBlock defined to simplify this section
        self.stem = nn.ModuleList()
        self.stem.append(nn.Conv2d(data_num_channels,widths[0],kernel_size=3,stride=1,padding=1,groups=1,bias=False))
        self.stem.append(nn.BatchNorm2d(widths[0])) # ??????????
        self.stem.append(nn.ReLU(inplace = True)) # ??????????

        self.body_1 = nn.ModuleList()
        self.body_1.append(XBlock(widths[0],widths[0],1,8))
        for _ in range(depths[0] - 1):
            self.body_1.append(XBlock(widths[0],widths[0],1,8))

        self.body_2 = nn.ModuleList()
        self.body_2.append(XBlock(widths[0],widths[1],2,8))
        for _ in range(depths[1] - 1):
            self.body_2.append(XBlock(widths[1],widths[1],1,8))

        self.body_3 = nn.ModuleList()
        self.body_3.append(XBlock(widths[1],widths[2],2,8))
        for _ in range(depths[2] - 1):
            self.body_3.append(XBlock(widths[2],widths[2],1,8))

        self.body_4 = nn.ModuleList()
        self.body_4.append(XBlock(widths[2],widths[3],2,8))
        for _ in range(depths[3] - 1):
            self.body_4.append(XBlock(widths[3],widths[3],1,8))

        self.avg = nn.AdaptiveAvgPool2d((1,1))
        self.fc = nn.Linear(widths[3],data_num_classes)

        self.apply(weight_initialization)

    # forward path
    def forward(self, x):

        # add your code here
        # tie together the operations to create a modified RegNetX-200MF
        y = x

        for layer in self.stem:
            y = layer(y)
            
        for layer in self.body_1:
            y = layer(y)
            
        for layer in self.body_2:
            y = layer(y)
        
        for layer in self.body_3:
            y = layer(y)
        
        for layer in self.body_4:
            y = layer(y)
        
        y = self.avg(y)
        y = y.view(y.size(0), -1)
        y = self.fc(y)

        # return
        return y


# create
model = Model(DATA_NUM_CHANNELS,
              # include model parameters here
              DEPTHS,
              WIDTHS,
              DATA_NUM_CLASSES)

# print(model)

# enable data parallelization for multi GPU systems
if (torch.cuda.device_count() > 1):
    model = nn.DataParallel(model)
print('Using {0:d} GPU(s)'.format(torch.cuda.device_count()), flush=True)

def calc_complexity(data_num_channels,depths,widths):
    """
    Computes the complexity of all the convolution layers in the class

    custom function that computes the MACs and Params of all the convolution layers in the pseudocode representation of the RegNetX-200MF

    Parameters:
        data_num_channels (int): starting number of channels of the input 
        depths (List[int]): list of depths used for the RegNetX-200MF model
        widths (List[int]): list of widths used for the RegNetX-200MF model

    Returns:
        None
    """
    cx = {
            "h": 56.0,
            "w": 56.0,
            "macs": 0.0,
            "params": 0.0
    }
    cx = conv2d_cx(cx,data_num_channels,widths[0],kernel_size=3) # 1
    # self.body_1
    cx = conv2d_cx(cx, widths[0], widths[0], kernel_size=1) # 2
    cx = conv2d_cx(cx, widths[0], widths[0], kernel_size=3,stride=1,groups=8) # 3
    cx = conv2d_cx(cx, widths[0], widths[0], kernel_size=1) # 4
    # self.body_2
    cx = conv2d_cx(cx, widths[0], widths[1], kernel_size=1) # 5
    cx = conv2d_cx(cx, widths[1], widths[1], kernel_size=3,stride=2,groups=8) # 6
    cx = conv2d_cx(cx, widths[1], widths[1], kernel_size=1) # 7
    # self.body_3
    cx = conv2d_cx(cx, widths[1], widths[2], kernel_size=1) # 8 
    cx = conv2d_cx(cx, widths[2], widths[2], kernel_size=3,stride=2,groups=8) # 9
    cx = conv2d_cx(cx, widths[2], widths[2], kernel_size=1) # 10 
    # widths[2],widths[2],1,8
    for _ in range(depths[2] - 1):
        cx = conv2d_cx(cx, widths[2], widths[2], kernel_size=1) # 11
        cx = conv2d_cx(cx, widths[2], widths[2], kernel_size=3,stride=1,groups=8) # 12
        cx = conv2d_cx(cx, widths[2], widths[2], kernel_size=1) # 13
    # self.body_4
    cx = conv2d_cx(cx, widths[2], widths[3], kernel_size=1) # 14
    cx = conv2d_cx(cx, widths[3], widths[3], kernel_size=3,stride=2,groups=8) #15
    cx = conv2d_cx(cx, widths[3], widths[3], kernel_size=1) # 16
    # widths[3],widths[3],1,8
    for _ in range(depths[3] - 1):
        cx = conv2d_cx(cx, widths[3], widths[3], kernel_size=1) # 17
        cx = conv2d_cx(cx, widths[3], widths[3], kernel_size=3,stride=1,groups=8) # 18 
        cx = conv2d_cx(cx, widths[3], widths[3], kernel_size=1) # 19
    print(cx["macs"], cx["params"])

calc_complexity(DATA_NUM_CHANNELS,DEPTHS,WIDTHS)

macs, params = profile(model, inputs=(input, ))

print(macs, params)

macs, params = clever_format([macs, params], "%.3f")

print(macs,params)

################################################################################
#
# ERROR AND OPTIMIZER
#
################################################################################

# add your code here
# define the error criteria and optimizer
# learning rate schedule
def lr_schedule(epoch):

    # linear warmup followed by cosine decay
    if epoch < TRAINING_LR_INIT_EPOCHS:
        lr = (TRAINING_LR_MAX - TRAINING_LR_INIT)*(float(epoch)/TRAINING_LR_INIT_EPOCHS) + TRAINING_LR_INIT
    else:
        lr = (TRAINING_LR_MAX - TRAINING_LR_FINAL)*max(0.0, math.cos(((float(epoch) - TRAINING_LR_INIT_EPOCHS)/(TRAINING_LR_FINAL_EPOCHS - 1.0))*(math.pi/2.0))) + TRAINING_LR_FINAL

    return lr

# error (softmax cross entropy)
criterion = nn.CrossEntropyLoss()

# optimizer
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)

# specify the device as the GPU if present with fallback to the CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
# print(device)

# transfer the network to the device
model.to(device)

# model loading
if FILE_LOAD == 1:
    checkpoint = torch.load(FILE_NAME)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1

################################################################################
#
# TRAINING
#
################################################################################

# add your code here
# perform network training, validation and checkpoint saving
# see previous examples in the Code directory
# cycle through the epochs
start_epoch = 0
for epoch in range(start_epoch, TRAINING_NUM_EPOCHS):

    # initialize train set statistics
    model.train()
    training_loss = 0.0
    num_batches   = 0

    # set the learning rate for the epoch
    for g in optimizer.param_groups:
        g['lr'] = lr_schedule(epoch)

    # cycle through the train set
    for data in tqdm(dataloader_train):

        # extract a batch of data and move it to the appropriate device
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward pass, loss, backward pass and weight update
        outputs = model(inputs)
        loss    = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # update statistics
        training_loss = training_loss + loss.item()
        num_batches   = num_batches + 1

    # initialize test set statistics
    model.eval()
    test_correct = 0
    test_total   = 0

    # no weight update / no gradient needed
    with torch.no_grad():

        # cycle through the test set
        for data in tqdm(dataloader_test):

            # extract a batch of data and move it to the appropriate device
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)

            # forward pass and prediction
            outputs      = model(inputs)
            _, predicted = torch.max(outputs.data, 1)

            # update test set statistics
            test_total   = test_total + labels.size(0)
            test_correct = test_correct + (predicted == labels).sum().item()

    # epoch statistics
    print('Epoch {0:2d} lr = {1:8.6f} avg loss = {2:8.6f} accuracy = {3:5.2f}'.format(epoch, lr_schedule(epoch), (training_loss/num_batches)/DATA_BATCH_SIZE, (100.0*test_correct/test_total)))

# model saving
# to use this for checkpointing put this code block inside the training loop at the end (e.g., just indent it 4 spaces)
# and set 'epoch' to the current epoch instead of TRAINING_NUM_EPOCHS - 1; then if there's a crash it will be possible
# to load this checkpoint and restart training from the last complete epoch instead of having to start training at the
# beginning
    if FILE_SAVE == 1:
        torch.save({
            'epoch': epoch - 1,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict()
        }, FILE_NAME)

################################################################################
#
# TEST
#
################################################################################
# initialize test set statistics
model.eval()
test_correct = 0
test_total   = 0

# initialize class statistics
class_correct = list(0. for i in range(DATA_NUM_CLASSES))
class_total   = list(0. for i in range(DATA_NUM_CLASSES))

# no weight update / no gradient needed
with torch.no_grad():

    # cycle through the test set
    for data in dataloader_test:

        # extract a batch of data and move it to the appropriate device
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)

        # forward pass and prediction
        outputs      = model(inputs)
        _, predicted = torch.max(outputs.data, 1)

        # update test set statistics
        test_total   = test_total + labels.size(0)
        test_correct = test_correct + (predicted == labels).sum().item()

        # update class statistics
        c = (predicted == labels).squeeze()
        for i in range(labels.size(0)):
            label                 = labels[i]
            class_correct[label] += c[i].item()
            class_total[label]   += 1

# test set statistics
print('Accuracy of test set = {0:5.2f}'.format((100.0*test_correct/test_total)))
print('')

# class statistics
for i in range(DATA_NUM_CLASSES):
    print('Accuracy of {0:5s}    = {1:5.2f}'.format(str(i), (100.0*class_correct[i]/(class_total[i] + 0.0000001))))

lossL = [
         0.008872,
         0.007443,
         0.006479,
         0.005870,
         0.005404,
         0.005038,
         0.004672,
         0.004385,
         0.004164,
         0.003974,
         0.003803,
         0.003687,
         0.003548,
         0.003435,
         0.003359,
         0.003243,
         0.003175,
         0.003093,
         0.003016,
         0.002948,
         0.002888,
         0.002829,
         0.002768,
         0.002705,
         0.002652,
         0.002606,
         0.002555,
         0.002505,
         0.002453,
         0.002407,
         0.002355,
         0.002321,
         0.002263,
         0.002245,
         0.002183,
         0.002149,
         0.002108,
         0.002053,
         0.002032,
         0.001983,
         0.001949,
         0.001906,
         0.001881,
         0.001832,
         0.001786,
         0.001759,
         0.001739,
         0.001700,
         0.001673,
         0.001656,      
]
accuracyL = [
             4.19,
             14.93,
             24.76,
             30.21,
             34.79,
             35.35,
             40.43,
             45.94,
             48.33,
             47.94,
             51.06,
             52.37,
             52.80,
             55.69,
             53.73,
             57.18,
             57.60,
             56.45,
             56.49,
             60.46,
             61.83,
             60.37,
             61.24,
             59.92,
             62.67,
             62.41,
             63.37,
             63.39,
             62.93,
             64.13,
             65.21,
             65.28,
             66.04,
             64.47,
             65.06,
             66.45,
             66.51,
             66.47,
             65.78,
             66.15,
             67.30,
             66.62,
             66.62,
             68.51,
             68.08,
             67.56,
             67.99,
             68.01,
             68.27,
             68.32
]
print(len(lossL), len(accuracyL))
plt.title('accuracy vs epoch')
plt.plot(range(len(accuracyL)),accuracyL, label= "Accuracy")
# plt.plot(range(len(lossL)),lossL, label= "AvgLoss")
plt.xlabel('Epoch')
plt.ylabel('accuracy')
plt.show()
plt.title('accuracy vs epoch')
# plt.plot(range(len(accuracyL)),accuracyL, label= "Accuracy")
plt.plot(range(len(lossL)),lossL, label= "AvgLoss")
plt.xlabel('Epoch')
plt.ylabel('AvgLoss')
plt.show()