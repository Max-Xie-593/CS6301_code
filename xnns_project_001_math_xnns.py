# -*- coding: utf-8 -*-
"""xNNs_Project_001_Math_XNNs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_BMEd3ld3ioHLN0sz8by8c7RpssCzceG
"""

################################################################################
#
# LOGISTICS
#
#    Your name as in eLearning
#    Your UT Dallas identifier
#
# DESCRIPTION
#
#    MNIST image classification with an xNN written and trained in Python
#
# INSTRUCTIONS
#
#    1. Go to Google Colaboratory: https://colab.research.google.com/notebooks/welcome.ipynb
#    2. File - New Python 3 notebook
#    3. Cut and paste this file into the cell (feel free to divide into multiple cells)
#    4. Runtime - Run all
#
# NOTES
#
#    1. This does not use PyTorch, TensorFlow or any other xNN library
#
#    2. Include a short summary here in nn.py of what you did for the neural
#       network portion of code
#
#    3. Include a short summary here in cnn.py of what you did for the
#       convolutional neural network portion of code
#
#    4. Include a short summary here in extra.py of what you did for the extra
#       portion of code
#
################################################################################

################################################################################
#
# IMPORT
#
################################################################################

#
# you should not need any import beyond the below
# PyTorch, TensorFlow, ... is not allowed
#

import os.path
import urllib.request
import gzip
import math
import numpy             as np
import matplotlib.pyplot as plt
from scipy.special import softmax
from scipy.stats import entropy
from abc import ABCMeta
from functools import wraps
from time import time
from tqdm.notebook import tqdm

################################################################################
#
# PARAMETERS
#
################################################################################

#
# add other hyper parameters here with some logical organization
#

# data
DATA_NUM_TRAIN         = 60000
DATA_NUM_TEST          = 10000
DATA_CHANNELS          = 1
DATA_ROWS              = 28
DATA_COLS              = 28
DATA_CLASSES           = 10
DATA_URL_TRAIN_DATA    = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'
DATA_URL_TRAIN_LABELS  = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'
DATA_URL_TEST_DATA     = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'
DATA_URL_TEST_LABELS   = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'
DATA_FILE_TRAIN_DATA   = 'train_data.gz'
DATA_FILE_TRAIN_LABELS = 'train_labels.gz'
DATA_FILE_TEST_DATA    = 'test_data.gz'
DATA_FILE_TEST_LABELS  = 'test_labels.gz'

# display
DISPLAY_ROWS   = 8
DISPLAY_COLS   = 4
DISPLAY_COL_IN = 10
DISPLAY_ROW_IN = 25
DISPLAY_NUM    = DISPLAY_ROWS*DISPLAY_COLS

################################################################################
#
# DATA
#
################################################################################

# download
if (os.path.exists(DATA_FILE_TRAIN_DATA)   == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_DATA,   DATA_FILE_TRAIN_DATA)
if (os.path.exists(DATA_FILE_TRAIN_LABELS) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_LABELS, DATA_FILE_TRAIN_LABELS)
if (os.path.exists(DATA_FILE_TEST_DATA)    == False):
    urllib.request.urlretrieve(DATA_URL_TEST_DATA,    DATA_FILE_TEST_DATA)
if (os.path.exists(DATA_FILE_TEST_LABELS)  == False):
    urllib.request.urlretrieve(DATA_URL_TEST_LABELS,  DATA_FILE_TEST_LABELS)

# training data
# unzip the file, skip the header, read the rest into a buffer and format to NCHW
file_train_data   = gzip.open(DATA_FILE_TRAIN_DATA, 'r')
file_train_data.read(16)
buffer_train_data = file_train_data.read(DATA_NUM_TRAIN*DATA_ROWS*DATA_COLS)
train_data        = np.frombuffer(buffer_train_data, dtype=np.uint8).astype(np.float32)
train_data        = train_data.reshape(DATA_NUM_TRAIN, 1, DATA_ROWS, DATA_COLS)

# training labels
# unzip the file, skip the header, read the rest into a buffer and format to a vector
file_train_labels   = gzip.open(DATA_FILE_TRAIN_LABELS, 'r')
file_train_labels.read(8)
buffer_train_labels = file_train_labels.read(DATA_NUM_TRAIN)
train_labels        = np.frombuffer(buffer_train_labels, dtype=np.uint8).astype(np.int32)

# testing data
# unzip the file, skip the header, read the rest into a buffer and format to NCHW
file_test_data   = gzip.open(DATA_FILE_TEST_DATA, 'r')
file_test_data.read(16)
buffer_test_data = file_test_data.read(DATA_NUM_TEST*DATA_ROWS*DATA_COLS)
test_data        = np.frombuffer(buffer_test_data, dtype=np.uint8).astype(np.float32)
test_data        = test_data.reshape(DATA_NUM_TEST, 1, DATA_ROWS, DATA_COLS)

# testing labels
# unzip the file, skip the header, read the rest into a buffer and format to a vector
file_test_labels   = gzip.open(DATA_FILE_TEST_LABELS, 'r')
file_test_labels.read(8)
buffer_test_labels = file_test_labels.read(DATA_NUM_TEST)
test_labels        = np.frombuffer(buffer_test_labels, dtype=np.uint8).astype(np.int32)

# debug
# print(train_data.shape)   # (60000, 1, 28, 28)
# print(train_labels.shape) # (60000,)
# print(test_data.shape)    # (10000, 1, 28, 28)
# print(test_labels.shape)  # (10000,)

def somax(vector):
  """
    Computes the softmax of the vector

    custom softmax that computes the softmax of the vector

    Parameters:
      vector (List[float]): List of the vectors

    Returns:
      float softmax of the vector
  """
  return np.exp(vector)/np.sum(np.exp(vector))


def get_one_hot(targets, nb_classes):
  """
    Converts the training labels to equivalent one-hot vectors

    Converts the training labels to equivalent one-hot vectors
    for convenience of the code

    Parameters:
      targets (List[int]): List of the Training Labels
      nb_classes (int): number of possible classes for the label

    Returns:
      List[List[int]]: List of one out vectors which are equivalent to 
      its respective label
  """
  res = np.array(np.eye(nb_classes)[np.array(targets).reshape(-1)])
  return res.reshape(list(targets.shape)+[1,nb_classes])

def cross_entropy_loss(x,y):
  """
    Computes the Cross Entropy Loss of x compared to y

    Computes the Cross Entropy Loss of the estimated label with respect to
    the corresponding true label

    Parameters:
      x (List[List[floats]]): softmax output of the neural network
      y (int) : true label of the image data

    returns: 
      float: cross_entropy of the parameters
  """
  return -np.log(x[y])

def timing(f):
  """
    Decorator for timimg purposes

    Decorator to time the execution of the function

    Parameters:
      f: function to decorate over

    Returns:
      the wrapped function (function it decorates over)
  """
  @wraps(f)
  def wrap(*args, **kw):
    """
    Decorator for timimg purposes

    Decorator to time the execution of the function

    Parameters:
      f: function to decorate over

    Returns:
      the wrapped function (function it decorates over)
    """
    ts = time()
    result = f(*args, **kw)
    te = time()
    print('func:{} args:[{},{}] took: {:2.4f} sec'.format(f.__name__, args, kw, te-ts))
    return result
  return wrap

class LayerInterface(metaclass=ABCMeta):
  """
    A peusdo Interface to ensure all derived classes contains certain methods for the Neural Network

    Methods
    -------
    __subclasshood__(cls, subclass):
        dunder method that checks if the subclass is a derived class of this class via
        checking if the class contains certain methods for the neural layers
  """
  @classmethod
  def __subclasshood__(cls, subclass):
    """
      Checks if the subclass is a 'correct' subclass of this Layer class.

      Checks if the subclass is a 'correct' subclass of this Layer class via
      checking if the class contains certain methods for the neural layers.

      Parameters
      ----------
      cls : class object
          class being compared to
      
      subclass: class object
          subclass being checked for 'correct' subclass

      Returns
      -------
      Boolean
    """
    return (hasattr(subclass, 'forward_pass') and hasattr(subclass, 'backward_prop') and hasattr(subclass, 'update_weight'))
    
class Division_Layer(LayerInterface):
  """
    Neural Layer to divide all the values in the data by a certain value

    Attributes
    ----------
    input_shape : Tuple[int]
        shape of the image_data

    output_shape : Tuple[int]
        shape of the image_data

    Methods
    -------
    __init__(self):
      initialization of the layer

    forward_pass(self, input_data):
      forward pass function for this neural layer

    backward_prop(self, error):
      backward propagation for this neural layer

    update_weight(self, learn_rate):
      updates the weights of this layer (if needed)
  """
  def __init__(self):
    """
        Constructs all the necessary attributes for the layer object.

        Parameters
        ----------
            None
    """
    self.input_shape = None
    self.output_shape = None
  
  def forward_pass(self, input_data):
    """
        Performs forward-pass on the data

        Parameters
        ----------
            input_data: NDArray[float]
              image data being passed through

        Returns
        -------
            NDArray[float]:
              image data divided by 255.0
    """
    self.input_shape = input_data.shape
    self.output_shape = input_data.shape
    return np.array(input_data / 255.0)

  def backward_prop(self, error):
    """
        Performs backward-propagation on the error of the data

        Parameters
        ----------
            error: NDArray[float]
              error data being passed back through this layer

        Returns
        -------
            NDArray[float]:
              returns error with no change since this layer has no need to 
              update weights
    """
    return error

  def update_weight(self,learn_rate):
    """
        updates the weights based on the learning_rate

        Parameters
        ----------
            learn_rate: float
              float which represents the learning_rate of the data

        Returns
        -------
            None:
              does nothing since there are no weights to update
    """
    pass

class Vectorization_Layer(LayerInterface):
  """
    Neural Layer to flatten all the values in the data to a single array

    Attributes
    ----------
    input_shape : Tuple[int]
        shape of the image_data
    
    output_shape : Tuple[int]
        shape of the image_data

    Methods
    -------
    __init__(self):
      initialization of the layer

    forward_pass(self, input_data):
      forward pass function for this neural layer

    backward_prop(self, error):
      backward propagation for this neural layer

    update_weight(self, learn_rate):
      updates the weights of this layer (if needed)
  """
  def __init__(self):
    """
        Constructs all the necessary attributes for the layer object.

        Parameters
        ----------
            None
    """
    self.input_shape = None
    self.output_shape = None

  def forward_pass(self, input_data):
    """
        Performs forward-pass on the data

        Parameters
        ----------
            input_data: NDArray[float]
              image data being passed through

        Returns
        -------
            NDArray[float]:
              image data flatten to a 1 by N NDArray
    """
    self.input_shape = input_data.shape
    return np.array([input_data.flatten()])

  def backward_prop(self, error):
    """
        Performs backward-propagation on the error of the data

        Parameters
        ----------
            error: NDArray[float]
              error data being passed back through this layer

        Returns
        -------
            NDArray[float]:
              returns error reshaped to original shape based on shape from
              forward_pass
    """
    self.output_shape = error.shape
    return np.reshape(error,self.input_shape)

  def update_weight(self,learn_rate):
    """
        updates the weights based on the learning_rate

        Parameters
        ----------
            learn_rate: float
              float which represents the learning_rate of the data

        Returns
        -------
            None:
              does nothing since there are no weights to update
    """
    pass
  
class Matrix_Multiplication_Layer(LayerInterface):
  """
    Neural Layer to perform Matrix multiplication

    Attributes
    ----------
    input : NDArray[float]
        cached data of the image data
      
    matrix : NDArray[float]
        matrix which contains the weights to muliply with the image_data
    
    hold: NDArray[float] 
        cached gradient of the weights need to update the weights

    input_shape : Tuple[int]
        shape of the image_data
    
    output_shape : Tuple[int]
        shape of the image_data

    Methods
    -------
    __init__(self):
      initialization of the layer

    forward_pass(self, input_data):
      forward pass function for this neural layer

    backward_prop(self, error):
      backward propagation for this neural layer

    update_weight(self, learn_rate):
      updates the weights of this layer (if needed)
  """
  def __init__(self, matrix_dims):
    """
        Constructs all the necessary attributes for the layer object.

        Parameters
        ----------
            matrix_dims: shape of the weight matrix
    """
    self.input = None
    self.matrix = np.random.rand(matrix_dims[0],matrix_dims[1]) / 100.0
    self.hold = None
    self.input_shape = None
    self.output_shape = None
  
  def forward_pass(self, input_data):
    """
        Performs forward-pass on the data

        Parameters
        ----------
            input_data: NDArray[float]
              image data being passed through

        Returns
        -------
            NDArray[float]:
              image data multiplied by the weight matrix
    """
    self.input = input_data
    self.input_shape = input_data.shape
    return np.matmul(input_data, self.matrix)

  def backward_prop(self, error):
    """
        Performs backward-propagation on the error of the data

        Parameters
        ----------
            error: NDArray[float]
              error data being passed back through this layer

        Returns
        -------
            NDArray[float]:
              returns error multiplied by transpose of the weight matrix
    """
    self.output_shape = error.shape
    self.hold = np.matmul(np.transpose(self.input),error)
    return np.matmul(error,np.transpose(self.matrix))

  def update_weight(self, learn_rate):
    """
        updates the weights based on the learning_rate

        Parameters
        ----------
            learn_rate: float
              float which represents the learning_rate of the data

        Returns
        -------
            None
    """
    self.matrix = self.matrix - (learn_rate * self.hold)

class Bias_Layer(LayerInterface):
  """
    Neural Layer to perform Addition

    Attributes
    ----------
    vector : NDArray[float]
        vector that holds the weights for the bias
    
    hold: NDArray[float] 
        cached gradient of the weights need to update the weights
    
    input_shape : Tuple[int]
        shape of the image_data

    output_shape : Tuple[int]
        shape of the image_data

    Methods
    -------
    __init__(self):
      initialization of the layer

    forward_pass(self, input_data):
      forward pass function for this neural layer

    backward_prop(self, error):
      backward propagation for this neural layer

    update_weight(self, learn_rate):
      updates the weights of this layer (if needed)
  """
  def __init__(self, vector_dims):
    """
        Constructs all the necessary attributes for the layer object.

        Parameters
        ----------
            vector_dims: shape of the weight vector
    """
    self.vector = np.random.rand(vector_dims[0],vector_dims[1]) / 100.0
    self.hold = None
    self.input_shape = None
    self.output_shape = None
  
  def forward_pass(self, input_data):
    """
        Performs forward-pass on the data

        Parameters
        ----------
            input_data: NDArray[float]
              image data being passed through

        Returns
        -------
            NDArray[float]:
              image data added by the weight vector
    """
    self.input_shape = input_data.shape
    self.output_shape = input_data.shape
    return input_data + self.vector

  def backward_prop(self, error):
    """
        Performs backward-propagation on the error of the data

        Parameters
        ----------
            error: NDArray[float]
              error data being passed back through this layer

        Returns
        -------
            NDArray[float]:
              returns error with no modifications
    """
    self.hold = error
    return error

  def update_weight(self, learn_rate):
    """
        updates the weights based on the learning_rate

        Parameters
        ----------
            learn_rate: float
              float which represents the learning_rate of the data

        Returns
        -------
            None
    """
    return self.vector - (learn_rate * self.hold)


class ReLu_Layer(LayerInterface):
  """
    Neural Layer to perform ReLu on the image_data

    Attributes
    ----------
    input : NDArray[float]
        vector that holds the weights for the bias

    input_shape : Tuple[int]
        shape of the image_data

    output_shape : Tuple[int]
        shape of the image_data

    Methods
    -------
    __init__(self):
      initialization of the layer

    forward_pass(self, input_data):
      forward pass function for this neural layer

    backward_prop(self, error):
      backward propagation for this neural layer

    update_weight(self, learn_rate):
      updates the weights of this layer (if needed)
  """
  def __init__(self):
    """
        Constructs all the necessary attributes for the layer object.

        Parameters
        ----------
            None
    """
    self.input = None
    self.input_shape = None
    self.output_shape = None
  
  def forward_pass(self, input_data):
    """
        Performs forward-pass on the data

        Parameters
        ----------
            input_data: NDArray[float]
              image data being passed through

        Returns
        -------
            NDArray[float]:
              image data pushed through via a reLu function
    """
    self.input = input_data
    self.input_shape = input_data.shape
    self.output_shape = input_data.shape
    # return np.maximum(input_data,0)
    return input_data * (input_data > 0)

  def backward_prop(self, error):
    """
        Performs backward-propagation on the error of the data

        Parameters
        ----------
            error: NDArray[float]
              error data being passed back through this layer

        Returns
        -------
            NDArray[float]:
              returns error with a matrix with 1 at same locations of 
              the original data
    """
    return error * (1 * (self.input > 0))

  def update_weight(self, learn_rate):
    """
        updates the weights based on the learning_rate

        Parameters
        ----------
            learn_rate: float
              float which represents the learning_rate of the data

        Returns
        -------
            None
    """
    pass

class Softmax_Layer(LayerInterface):
  """
    Neural Layer to perform Softmax on the image_data

    Attributes
    ----------
      input_shape : Tuple[int]
          shape of the image_data

      output_shape : Tuple[int]
        shape of the image_data
  
    Methods
    -------
    __init__(self):
      initialization of the layer

    forward_pass(self, input_data):
      forward pass function for this neural layer

    backward_prop(self, error):
      backward propagation for this neural layer

    update_weight(self, learn_rate):
      updates the weights of this layer (if needed)
  """
  def __init__(self):
    """
        Constructs all the necessary attributes for the layer object.

        Parameters
        ----------
            None
    """
    self.input_shape = None
    self.output_shape = None
  
  def forward_pass(self, input_data):
    """
        Performs forward-pass on the data

        Parameters
        ----------
            input_data: NDArray[float]
              image data being passed through

        Returns
        -------
            NDArray[float]:
              image data pushed through via a softmax function
    """
    self.input_shape = input_data.shape
    self.output_shape = input_data.shape
    return softmax(input_data)

  def backward_prop(self, error):
    """
        Performs backward-propagation on the error of the data

        Parameters
        ----------
            error: NDArray[float]
              error data being passed back through this layer

        Returns
        -------
            NDArray[float]:
              returns error with no modifications since the actual math for this
              back propagation is done at the error parameter
    """
    return error
  
  def update_weight(self, learn_rate):
    """
        updates the weights based on the learning_rate

        Parameters
        ----------
            learn_rate: float
              float which represents the learning_rate of the data

        Returns
        -------
            None
    """
    pass

class TraditionalNN(object):
  """
    Whole Neural Network that has all the layers for the specific NN

    Attributes
    ----------
    learning_rate : float
        float that holds the learning_rate
    
    layers: List[LayerInterface]
        List that holds all the needed layers to push the data through
  
    Methods
    -------
    __init__(self):
      initialization of the layer

    forward(self, input_data):
      forward pass function for this neural network

    backward(self, error):
      backward propagation for this neural network

    update(self, learn_rate):
      updates the weights of all the layers in the neural network (if needed)

    __str__(self):
      prints infomation on the layers in self.layers
  """
  def __init__(self, learn_rate):
    """
        Constructs all the necessary attributes for the layer object.

        Parameters
        ----------
            learn_rate: float
              learning_rate for the neural network
    """
    self.learning_rate = learn_rate
    self.layers = [
      Division_Layer(),
      Vectorization_Layer(),
      Matrix_Multiplication_Layer((784,1000)),
      Bias_Layer((1,1000)),
      ReLu_Layer(),
      Matrix_Multiplication_Layer((1000,100)),
      Bias_Layer((1,100)),
      ReLu_Layer(),
      Matrix_Multiplication_Layer((100,10)),
      Bias_Layer((1,10)),
      Softmax_Layer()
    ]
  
  def forward(self, input_data):
    """
        Performs forward-pass on the data

        Parameters
        ----------
            input_data: NDArray[float]
              image data being passed through

        Returns
        -------
            NDArray[float]:
              image data pushed through via a variety of layers
    """
    output = input_data
    for layer in self.layers:
      output = layer.forward_pass(output)
    return output

  def backward(self, input_data):
    """
        Performs backward-propagation on the error of the data

        Parameters
        ----------
            error: NDArray[float]
              error data being passed back through this layer

        Returns
        -------
            NDArray[float]:
              returns error pushed through a varity of layers
    """
    output = input_data
    for layer in reversed(self.layers):
      output = layer.backward_prop(output)
    return output

  def update(self):
    """
        updates the weights based on the learning_rate

        Parameters
        ----------
            learn_rate: float
              float which represents the learning_rate of the data

        Returns
        -------
            None
    """
    for layer in self.layers:
      layer.update_weight(self.learning_rate)
  
  def __str__(self):
    return '\n'.join(
        'layer: {:3} | type: {:30} | input size: {:20} | output size: {:20} | MACs: {}'.format(
            idx + 1, 
            type(layer).__name__,
            str(layer.input_shape),
            str(layer.output_shape),
            np.prod(layer.input_shape) * np.prod(layer.output_shape[1:])
            ) 
        for idx, layer in enumerate(self.layers)
    )

################################################################################
#
# YOUR CODE GOES HERE
#
################################################################################

#
# feel free to split this into some number of classes, functions, ... if it
# helps with code organization; for example, you may want to create a class for
# each of your layers that store parameters, performs initialization and
# includes forward and backward functions
#
@timing
def test():
  """
    test function for debug purposes

    testing function for debuggin purposes

    Parameters:

    Returns:
      int: sum of all the cross entropy loss for the training data
  """
  check = TraditionalNN(0.6)
  h = np.array([check.forward(data) for data in tqdm(train_data)])
  true_label = get_one_hot(train_labels,10) 
  return np.sum([cross_entropy_loss(x,y) for x,y in zip(h,true_label)])

NN = TraditionalNN(0.01)
true_training_label = get_one_hot(train_labels,10)

accuracyL = []
timeL = []
lossL = []

for x in range(10): # cycle through the epochs
    loss = 0
    ts = time()
    for idx, data in enumerate(tqdm(train_data[:10000])): # cycle through the training data
        estimated_label = NN.forward(data) # forward pass
        loss += cross_entropy_loss(estimated_label[0],train_labels[idx]) # loss
        # back prop
        NN.backward(estimated_label - true_training_label[idx]) # estimated_label - true_training_label[idx] IS THE BACK PROPAGATION FOR SOFTMAX
        NN.update() # weight update

    correct = 0
    for idx, Tdata in enumerate(tqdm(test_data)): # cycle through the testing data
        estimated_test = NN.forward(Tdata) # forward pass
        if test_labels[idx] == np.argmax(estimated_test): # accuracy
          correct += 1
    te = time()
    # per epoch display (epoch, time, training loss, testing accuracy, ...)
    timeL.append(te-ts)
    lossL.append(loss/10000)
    accuracyL.append(correct/10000)
    print('epoch: {:d} | time: {:2.4f} sec | loss: {} | test accuracy: {}'.format(x+1,te-ts,loss/60000,correct/10000))

################################################################################
#
# DISPLAY
#
################################################################################

#
# more code for you to write
#
estimated_test_labels = [NN.forward(Tdata) for idx, Tdata in enumerate(test_data)]

# accuracy display
print('Final Accuracy of the Neural Net: {}'.format(accuracyL[-1]))
# final value
# plot of accuracy vs epoch
plt.title('accuracy & values vs epoch (10000 train)')
plt.plot(range(len(accuracyL)),accuracyL, label= "Accuracy")
plt.plot(range(len(lossL)),lossL, label= "AvgLoss")
plt.xlabel('Epoch')
plt.ylabel('values')
plt.show()

plt.title('time vs epoch (10000 train)')
plt.plot(range(len(timeL)),timeL, label= "Time")
plt.xlabel('Epoch')
plt.ylabel('time')
plt.show()

# performance display
# total time
# per layer info (type, input size, output size, parameter size, MACs, ...)
print(NN)

# example display
# replace the xNN predicted label with the label predicted by the network
fig = plt.figure(figsize=(DISPLAY_COL_IN, DISPLAY_ROW_IN))
ax  = []
for i in range(DISPLAY_NUM):
    img = test_data[i, :, :, :].reshape((DATA_ROWS, DATA_COLS))
    ax.append(fig.add_subplot(DISPLAY_ROWS, DISPLAY_COLS, i + 1))
    ax[-1].set_title('True: ' + str(test_labels[i]) + ' xNN: ' + str(np.argmax(estimated_test_labels[i])))
    plt.imshow(img, cmap='Greys')
plt.show()